{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 비슷한 단어 모델 학습\n",
    "\n",
    "1. DataFrame을 만든다 -> crawling을 하던 (crawl_query) 또는 저장된 data를 불러와서 dataframe으로 만든다\n",
    "2. model_train을 한다. -> 결과: tokenized 단어 list + word2vec학습된 모델\n",
    "3. create_tensors를 하면 모델을 tensor로 바꾸고 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import gensim\n",
    "from gensim import utils\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def word2vec2tensor(word2vec_model, tensor_filename, binary=False):\n",
    "    \"\"\"Convert file in Word2Vec format and writes two files 2D tensor TSV file.\n",
    "\n",
    "    File \"tensor_filename\"_tensor.tsv contains word-vectors, \"tensor_filename\"_metadata.tsv contains words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word2vec_model_path : str\n",
    "        Path to file in Word2Vec format.\n",
    "    tensor_filename : str\n",
    "        Prefix for output files.\n",
    "    binary : bool, optional\n",
    "        True if input file in binary format.\n",
    "\n",
    "    \"\"\"\n",
    "    model = word2vec_model\n",
    "    outfiletsv = tensor_filename + '_tensor.tsv'\n",
    "    outfiletsvmeta = tensor_filename + '_metadata.tsv'\n",
    "\n",
    "    with utils.open(outfiletsv, 'wb') as file_vector, utils.open(outfiletsvmeta, 'wb') as file_metadata:\n",
    "        for word in model.index2word:\n",
    "            file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\\n'))\n",
    "            vector_row = '\\t'.join(str(x) for x in model[word])\n",
    "            file_vector.write(gensim.utils.to_utf8(vector_row) + gensim.utils.to_utf8('\\n'))\n",
    "\n",
    "    logger.info(\"2D tensor file saved to %s\", outfiletsv)\n",
    "    logger.info(\"Tensor metadata file saved to %s\", outfiletsvmeta)\n",
    "    \n",
    "def model_train(dataframe, content_col, size=100, window=5):\n",
    "    okt=Okt()\n",
    "    result = []\n",
    "    print('loading ', end='')\n",
    "    for index, row, in df.iterrows():\n",
    "        if (index % 1000 == 0):\n",
    "            print('. ', end='')\n",
    "        try:\n",
    "            tokenlist = okt.pos(row[content_col], stem=True, norm=True) # 단어 토큰화\n",
    "        except:\n",
    "            print(index, end='')\n",
    "            continue\n",
    "        temp=[]\n",
    "        for word in tokenlist:\n",
    "            if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "                temp.append((word[0])) # 해당 단어를 저장함\n",
    "\n",
    "        if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "            result.append(temp) # 결과에 저장\n",
    "    print('\\n\\nFinished!')\n",
    "    \n",
    "    model = Word2Vec(result, size=size, window=window, min_count=5, workers=4, sg=0)\n",
    "    return result, model\n",
    "\n",
    "def print_similar(model_, query):\n",
    "    try:\n",
    "        model_result = model_.wv.most_similar(query)\n",
    "        print(model_result)\n",
    "    except KeyError:\n",
    "        print('{} not in model vocabulary. Please try another query.'.format(query))\n",
    "    \n",
    "def create_tensors(model, output):\n",
    "    word2vec2tensor(model.wv, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def crawler(maxpage,query,s_date,e_date):\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1 # 11= 2페이지 21=3페이지 31=4페이지 ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    #f = open(RESULT_PATH + filename, 'w', encoding='utf-8-sig')\n",
    "    df = pd.DataFrame(columns=['date', 'title', 'contents'])\n",
    "    results_list = []\n",
    "\n",
    "    while page < maxpage_t:\n",
    "        print(page, 'loading', end='')\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        req = requests.get(url)\n",
    "        # print(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        #print(soup)\n",
    "        for urls in soup.select(\"._sp_each_url\"):\n",
    "            try :\n",
    "                #print(urls[\"href\"])\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    #print(urls[\"href\"])\n",
    "                    news_detail = get_news(urls[\"href\"])\n",
    "                    # pdate, pcompany, title, btext\n",
    "                    results_list.append((news_detail[1], news_detail[0], news_detail[2])) # date, title, contents\n",
    "                    #f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(news_detail[1], news_detail[4], news_detail[0], news_detail[2],news_detail[3])) # new style\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "        page += 10\n",
    "    df = pd.DataFrame(results_list, columns=['date', 'title', 'contents'])\n",
    "    print('\\nFinished!\\n')\n",
    "    return df\n",
    "\n",
    "def get_news(n_url):\n",
    "    print('.', end='')\n",
    "    news_detail = []\n",
    "    breq = requests.get(n_url)\n",
    "    bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "    title = bsoup.select('h3#articleTitle')[0].text #대괄호는 h3#articleTitle 인 것중 첫번째 그룹만 가져오겠다.\n",
    "    news_detail.append(title)\n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:11]\n",
    "    news_detail.append(pdate)\n",
    "    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "    news_detail.append(btext.strip())\n",
    "    news_detail.append(n_url)\n",
    "    pcompany = bsoup.select('#footer address')[0].a.get_text()\n",
    "    news_detail.append(pcompany)\n",
    "    # news_detail [title, pdate, btext, n_url, pcompany]\n",
    "    return news_detail\n",
    "\n",
    "def crawl_query(max_pages, query,s_date='2020.06.01', e_date='2020.06.02'):\n",
    "    maxpage = str(max_pages)\n",
    "    return crawler(maxpage,query,s_date,e_date)\n",
    "\n",
    "def crawl_n_save(max_pages, query,s_date='2020.06.01', e_date='2020.06.02'):\n",
    "    maxpage = str(max_pages)\n",
    "    df_temp = crawler(maxpage,query,s_date,e_date)\n",
    "    df_temp.to_pickle(\"./data_{}.pkl\".format(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loading............................11 loading..............21 loading.................31 loading...............41 loading............51 loading...................61 loading............71 loading...................81 loading...............91 loading...........101 loading..................111 loading.................121 loading...................131 loading..........141 loading............151 loading...............161 loading................171 loading..............181 loading...........191 loading............201 loading..............211 loading..............221 loading...........231 loading..........241 loading...............251 loading.............261 loading.............271 loading............281 loading.............291 loading...........301 loading..............311 loading...........321 loading..........331 loading................341 loading...........351 loading.............361 loading..........371 loading............381 loading...................391 loading..............401 loading..............411 loading.............421 loading.............431 loading..................441 loading...............451 loading.......461 loading..........471 loading.................481 loading..............\n",
      "Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = crawl_query(1000, '코로나', s_date='2020.04.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading . \n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "result, model = model_train(df, 'contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('하객', 0.9986556172370911), ('인증', 0.9983986616134644), ('흡연', 0.9982520341873169), ('개사', 0.9982223510742188), ('주년', 0.9979987144470215), ('부과', 0.9979167580604553), ('감사', 0.9978440403938293), ('오', 0.9977390170097351), ('메이', 0.9977235794067383), ('아시아나', 0.99772047996521)]\n"
     ]
    }
   ],
   "source": [
    "print_similar(model, '대면')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020.06.02.</td>\n",
       "      <td>도쿄 코로나19 신규 확진 34명…'도쿄 경보' 첫 발령(종합2보)</td>\n",
       "      <td>일본 전체 신규 감염자 51명…누적 1만7천712명(도쿄=연합뉴스) 박세진 특파원 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020.06.02.</td>\n",
       "      <td>도쿄, 19일만에 코로나19 신규 확진 30명 넘어…'도쿄 경보' 발령 검토</td>\n",
       "      <td>일본 수도인 도쿄도(東京都)에서 2일 하루 동안 34명이 새롭게 신종 코로나바이러스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020.06.02.</td>\n",
       "      <td>도쿄, 코로나 확진자 2주만에 30명대...'제2파' 우려</td>\n",
       "      <td>긴급사태 해제 직후인 지난달 26일 도쿄의 출근길 표정. AP뉴시스 【도쿄=조은효 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020.06.02.</td>\n",
       "      <td>도쿄 코로나 신규 확진 34명…'도쿄 경보' 발령 검토(종합)</td>\n",
       "      <td>일본 긴급사태 해제 첫날 붐비는 지하철역(도쿄 로이터=연합뉴스) 일본의 신종 코로나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020.06.02.</td>\n",
       "      <td>日도쿄, ‘도쿄 경보’ 첫 발령…코로나 확진자 34명</td>\n",
       "      <td>[서울신문]일본 전역에서 코로나19에 따른 긴급사태가 해제된 가운데 26일 일본 도...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                       title  \\\n",
       "0  2020.06.02.       도쿄 코로나19 신규 확진 34명…'도쿄 경보' 첫 발령(종합2보)   \n",
       "1  2020.06.02.  도쿄, 19일만에 코로나19 신규 확진 30명 넘어…'도쿄 경보' 발령 검토   \n",
       "2  2020.06.02.            도쿄, 코로나 확진자 2주만에 30명대...'제2파' 우려   \n",
       "3  2020.06.02.          도쿄 코로나 신규 확진 34명…'도쿄 경보' 발령 검토(종합)   \n",
       "4  2020.06.02.               日도쿄, ‘도쿄 경보’ 첫 발령…코로나 확진자 34명   \n",
       "\n",
       "                                            contents  \n",
       "0  일본 전체 신규 감염자 51명…누적 1만7천712명(도쿄=연합뉴스) 박세진 특파원 ...  \n",
       "1  일본 수도인 도쿄도(東京都)에서 2일 하루 동안 34명이 새롭게 신종 코로나바이러스...  \n",
       "2  긴급사태 해제 직후인 지난달 26일 도쿄의 출근길 표정. AP뉴시스 【도쿄=조은효 ...  \n",
       "3  일본 긴급사태 해제 첫날 붐비는 지하철역(도쿄 로이터=연합뉴스) 일본의 신종 코로나...  \n",
       "4  [서울신문]일본 전역에서 코로나19에 따른 긴급사태가 해제된 가운데 26일 일본 도...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('test_corona_df.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading . . . . \n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "result, model = model_train(df, 'contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('교육', 0.8222788572311401), ('강의', 0.8144059181213379), ('이수', 0.7947985529899597), ('무상급식', 0.786588191986084), ('접근성', 0.7861039042472839), ('거나', 0.7815482020378113), ('격', 0.7808001637458801), ('체험학습', 0.7588086724281311), ('직업', 0.7583791017532349), ('형태', 0.7552222609519958)]\n"
     ]
    }
   ],
   "source": [
    "print_similar(model, '대면')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfList = os.listdir(\"./\"+save_dir+\"/\")\\npkl_list = [file for file in fList if file.endswith(\".pkl\")]\\n\\nfor i in range(len(pkl_list)):\\n    pklName = \"./\"+save_dir+\"/\"+pkl_list[i]\\n    model_name = \"./res_word2vec/model_\" + query + \"_\" + pklName[-7:-4]\\n    df = pd.read_pickle(pklName)\\n    result, model = model_train(df, \\'contents\\')\\n    create_tensors(model, model_name)\\n    for j in range(len(word_similar_set)):            \\n            word_similar = word_similar_set[j]\\n            print(\"Get similarity for \" + word_similar)\\n            print_similar(model, word_similar)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import gensim\n",
    "from gensim import utils\n",
    "import csv\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "datafile = pd.read_csv('Stopwords_Korean.csv')\n",
    "stopWordsKorean = datafile['words']\n",
    "\n",
    "\n",
    "def word2vec2tensor(word2vec_model, tensor_filename, binary=False):\n",
    "    \"\"\"Convert file in Word2Vec format and writes two files 2D tensor TSV file.\n",
    "\n",
    "    File \"tensor_filename\"_tensor.tsv contains word-vectors, \"tensor_filename\"_metadata.tsv contains words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word2vec_model_path : str\n",
    "        Path to file in Word2Vec format.\n",
    "    tensor_filename : str\n",
    "        Prefix for output files.\n",
    "    binary : bool, optional\n",
    "        True if input file in binary format.\n",
    "\n",
    "    \"\"\"\n",
    "    model = word2vec_model\n",
    "    outfiletsv = tensor_filename + '_tensor.tsv'\n",
    "    outfiletsvmeta = tensor_filename + '_metadata.tsv'\n",
    "\n",
    "    with utils.open(outfiletsv, 'wb') as file_vector, utils.open(outfiletsvmeta, 'wb') as file_metadata:\n",
    "        for word in model.index2word:\n",
    "            file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\\n'))\n",
    "            vector_row = '\\t'.join(str(x) for x in model[word])\n",
    "            file_vector.write(gensim.utils.to_utf8(vector_row) + gensim.utils.to_utf8('\\n'))\n",
    "\n",
    "    logger.info(\"2D tensor file saved to %s\", outfiletsv)\n",
    "    logger.info(\"Tensor metadata file saved to %s\", outfiletsvmeta)\n",
    "    \n",
    "def model_train(dataframe, content_col, size=100, window=5):\n",
    "    okt=Okt()\n",
    "    result = []\n",
    "    print('loading ', end='')\n",
    "    for index, row, in df.iterrows():\n",
    "        if (index % 100 == 0):\n",
    "            print('. ', end='')\n",
    "        try:\n",
    "            tokenlist = okt.pos(row[content_col], stem=True, norm=True) # 단어 토큰화\n",
    "        except:\n",
    "            print(index, end='')\n",
    "            continue\n",
    "        temp=[]\n",
    "        \n",
    "        for word in tokenlist:\n",
    "            if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "                temp.append((word[0])) # 해당 단어를 저장함\n",
    "\n",
    "        if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "            result.append(temp) # 결과에 저장\n",
    "    print('\\n\\nFinished!')\n",
    "    \n",
    "    text = []\n",
    "    for i in range(len(result)):\n",
    "        text.append([word for word in result[i] if word not in stopWordsKorean])\n",
    "        # print(str(len(result[i])-len(text[i])) + \" stopwords are eliminated.\")\n",
    "        \n",
    "    model = Word2Vec(text, size=size, window=window, min_count=5, workers=4, sg=0)\n",
    "    return result, model\n",
    "\n",
    "def print_similar(model_, query):\n",
    "    try:\n",
    "        model_result = model_.wv.most_similar(query)\n",
    "        print(model_result)\n",
    "        return model_result\n",
    "    except KeyError:\n",
    "        print('{} not in model vocabulary. Please try another query.'.format(query))\n",
    "    \n",
    "def create_tensors(model, output):\n",
    "    word2vec2tensor(model.wv, output)\n",
    "    \n",
    "    \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def crawler(maxpage, query, s_date, e_date, week, save_dir):\n",
    "    #maxpage = str(max_pages)\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1 # 11= 2페이지 21=3페이지 31=4페이지 ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    #f = open(RESULT_PATH + filename, 'w', encoding='utf-8-sig')\n",
    "    df = pd.DataFrame(columns=['date', 'title', 'contents'])\n",
    "    results_list = []\n",
    "\n",
    "    while page < maxpage_t:\n",
    "        print(page, 'loading', end='')\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        req = requests.get(url)\n",
    "        # print(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        #print(soup)\n",
    "        for urls in soup.select(\"._sp_each_url\"):\n",
    "            try :\n",
    "                #print(urls[\"href\"])\n",
    "                # num_chkin = 0 #tmp aks 200609\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    # num_chkin += 1\n",
    "                    #print(urls[\"href\"])\n",
    "                    # news_detail = get_news(urls[\"href\"])\n",
    "                    n_url = urls[\"href\"]\n",
    "                    print('.', end='')\n",
    "                    news_detail = []\n",
    "                    breq = requests.get(n_url)\n",
    "                    bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "                    title = bsoup.select('h3#articleTitle')[0].text #대괄호는 h3#articleTitle 인 것중 첫번째 그룹만 가져오겠다.\n",
    "                    news_detail.append(title)\n",
    "                    pdate = bsoup.select('.t11')[0].get_text()[:11]\n",
    "                    news_detail.append(pdate)\n",
    "                    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "                    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "                    news_detail.append(btext.strip())\n",
    "                    news_detail.append(n_url)\n",
    "                    pcompany = soup.select('#footer address')[0].a.get_text()\n",
    "                    news_detail.append(pcompany)\n",
    "                    # pdate, pcompany, title, btext\n",
    "                    results_list.append((news_detail[1], news_detail[0], news_detail[2])) # date, title, contents\n",
    "                    #f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(news_detail[1], news_detail[4], news_detail[0], news_detail[2],news_detail[3])) # new style\n",
    "                # else:\n",
    "                #     break\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "        page += 10\n",
    "    df = pd.DataFrame(results_list, columns=['date', 'title', 'contents'])\n",
    "    for i in range(len(df)):\n",
    "        # print(df.contents[i])\n",
    "        while \"기자\" not in df.contents[i] and \"특파원\" in df.contents[i] and \"@\" in df.contents[i]:\n",
    "            if \"기자\" in df.contents[i]:\n",
    "                ind_del = df.contents[i].rindex(\"기자\")\n",
    "                if ind_del/len(df.contents[i]) < 0.7:\n",
    "                    ind_del = len(df.contents[i])\n",
    "                    df.contents[i] = df.contents[i][0:ind_del-30]\n",
    "                else:\n",
    "                    df.contents[i] = df.contents[i][0:ind_del-10]\n",
    "            elif \"특파원\" in df.contents[i]:\n",
    "                ind_del = df.contents[i].rindex(\"특파원\")\n",
    "                if ind_del/len(df.contents[i]) < 0.7:\n",
    "                    ind_del = len(df.contents[i])\n",
    "                    df.contents[i] = df.contents[i][0:ind_del-30]\n",
    "                else:\n",
    "                    df.contents[i] = df.contents[i][0:ind_del-10]\n",
    "            elif \"@\" in df.contents[i]:\n",
    "                ind_del = df.contents[i].rindex(\"@\")\n",
    "                if ind_del/len(df.contents[i]) < 0.7:\n",
    "                    ind_del = len(df.contents[i])\n",
    "                    df.contents[i] = df.contents[i][0:ind_del-30]\n",
    "                else:\n",
    "                    df.contents[i] = df.contents[i][0:ind_del-10]\n",
    "            else:\n",
    "                ind_del = len(df.contents[i])\n",
    "                df.contents[i] = df.contents[i][0:ind_del-30]\n",
    "            # print(df.contents[i])\n",
    "    df.to_pickle(\"./\"+save_dir+\"/data_{}_W{}.pkl\".format(query,week))\n",
    "    print('\\nFinished!\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "''''''''''''''''''''''''''' Settings & RUN '''''''''''''''''''''''''''''\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "save_dir = 'res_naverCrawl'\n",
    "\n",
    "s_date_set = [\"2019.06.01\",\"2019.06.08\",\"2019.06.15\",\"2019.06.22\",\"2019.06.29\", \\\n",
    "          \"2019.07.06\",\"2019.07.13\",\"2019.07.20\",\"2019.07.27\",\"2019.08.03\", \\\n",
    "          \"2019.08.10\",\"2019.08.17\",\"2019.08.24\",\"2019.08.31\",\"2019.09.07\", \\\n",
    "          \"2019.09.14\",\"2019.09.21\",\"2019.09.28\",\"2019.10.05\",\"2019.10.12\", \\\n",
    "          \"2019.10.19\",\"2019.10.26\",\"2019.11.02\",\"2019.11.09\",\"2019.11.16\", \\\n",
    "          \"2019.11.23\",\"2019.11.30\",\"2019.12.07\",\"2019.12.14\",\"2019.12.21\", \\\n",
    "          \"2019.12.28\",\"2020.01.04\",\"2020.01.11\",\"2020.01.18\",\"2020.01.25\", \\\n",
    "          \"2020.02.01\",\"2020.02.08\",\"2020.02.15\",\"2020.02.22\",\"2020.02.29\", \\\n",
    "          \"2020.03.07\",\"2020.03.14\",\"2020.03.21\",\"2020.03.28\",\"2020.04.04\", \\\n",
    "          \"2020.04.11\",\"2020.04.18\",\"2020.04.25\",\"2020.05.02\",\"2020.05.09\", \\\n",
    "          \"2020.05.16\",\"2020.05.23\"]\n",
    "\n",
    "e_date_set = [\"2019.06.07\",\"2019.06.14\",\"2019.06.21\",\"2019.06.28\",\"2019.07.05\", \\\n",
    "          \"2019.07.12\",\"2019.07.19\",\"2019.07.26\",\"2019.08.02\",\"2019.08.09\", \\\n",
    "          \"2019.08.16\",\"2019.08.23\",\"2019.08.30\",\"2019.09.06\",\"2019.09.13\", \\\n",
    "          \"2019.09.20\",\"2019.09.27\",\"2019.10.04\",\"2019.10.11\",\"2019.10.18\", \\\n",
    "          \"2019.10.25\",\"2019.11.01\",\"2019.11.08\",\"2019.11.15\",\"2019.11.22\", \\\n",
    "          \"2019.11.29\",\"2019.12.06\",\"2019.12.13\",\"2019.12.20\",\"2019.12.27\", \\\n",
    "          \"2020.01.03\",\"2020.01.10\",\"2020.01.17\",\"2020.01.24\",\"2020.01.31\", \\\n",
    "          \"2020.02.07\",\"2020.02.14\",\"2020.02.21\",\"2020.02.28\",\"2020.03.06\", \\\n",
    "          \"2020.03.13\",\"2020.03.20\",\"2020.03.27\",\"2020.04.03\",\"2020.04.10\", \\\n",
    "          \"2020.04.17\",\"2020.04.24\",\"2020.05.01\",\"2020.05.08\",\"2020.05.15\", \\\n",
    "          \"2020.05.22\",\"2020.05.29\"]\n",
    "    \n",
    "maxpage = '100'\n",
    "query = ['인공지능', '금융', '산업']\n",
    "word_similar_set = [\"대면\",\"ZOOM\",\"zoom\",\"원격수업\",\"원격진료\",\"온라인\",\"기업\", \\\n",
    "                \"구글\",\"삼성\",\"네이버\",\"카카오\",\"삼성 SDS\",\"페이스북\",\"LG CNS\", \\\n",
    "                \"아마존\",\"AWS\",\"트위터\",\"넷플릭스\",\"배달의민족\",\"요기요\",\"우아한형제들\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for i in range(len(s_date_set)):\n",
    "    for j in query:\n",
    "        week = str(i+1).zfill(2)\n",
    "        s_date = s_date_set[i]\n",
    "        e_date = e_date_set[i]\n",
    "        df = crawler(maxpage, j, s_date, e_date, week, save_dir)\n",
    "        print(\"Crawling.....\"+week+\"/\"+str(len(s_date_set)))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "fList = os.listdir(\"./\"+save_dir+\"/\")\n",
    "pkl_list = [file for file in fList if file.endswith(\".pkl\")]\n",
    "\n",
    "for i in range(len(pkl_list)):\n",
    "    pklName = \"./\"+save_dir+\"/\"+pkl_list[i]\n",
    "    model_name = \"./res_word2vec/model_\" + query + \"_\" + pklName[-7:-4]\n",
    "    df = pd.read_pickle(pklName)\n",
    "    result, model = model_train(df, 'contents')\n",
    "    create_tensors(model, model_name)\n",
    "    for j in range(len(word_similar_set)):            \n",
    "            word_similar = word_similar_set[j]\n",
    "            print(\"Get similarity for \" + word_similar)\n",
    "            print_similar(model, word_similar)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training & Saving \"data_금융_W01.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W02.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W03.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W04.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W05.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W06.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W07.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W08.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W09.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W10.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W11.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W12.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W13.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W14.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W15.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W16.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W17.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W18.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W19.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W20.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W21.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W22.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W23.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W24.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W25.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W26.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W27.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W28.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W29.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W30.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W31.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W32.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W33.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W34.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W35.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W36.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W37.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W38.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W39.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W40.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W41.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W42.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W43.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W44.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W45.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W46.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W47.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W48.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W49.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W50.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W51.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_금융_W52.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W01.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W02.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W03.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W04.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W05.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W06.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W07.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W08.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W09.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W10.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W11.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W12.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W13.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W14.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W15.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W16.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W17.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W18.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W19.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W20.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W21.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W22.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W23.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W24.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W25.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W26.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W27.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W28.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W29.pkl\" dataframe...\n",
      "loading . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W30.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W31.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W32.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W33.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W34.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W35.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W36.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W37.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W38.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W39.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W40.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W41.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W42.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W43.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W44.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W45.pkl\" dataframe...\n",
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W46.pkl\" dataframe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W47.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W48.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W49.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W50.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W51.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_산업_W52.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W01.pkl\" dataframe...\n",
      "loading . . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W02.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W03.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W04.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W05.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W06.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W07.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W08.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W09.pkl\" dataframe...\n",
      "loading . . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W10.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W11.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W12.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W13.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W14.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W15.pkl\" dataframe...\n",
      "loading . . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W16.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W17.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W18.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W19.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W20.pkl\" dataframe...\n",
      "loading . . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W21.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W22.pkl\" dataframe...\n",
      "loading . . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W23.pkl\" dataframe...\n",
      "loading . . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W24.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W25.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W26.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W27.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W28.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W29.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W30.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W31.pkl\" dataframe...\n",
      "loading . . . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W32.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W33.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W34.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W35.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W36.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W37.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W38.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W39.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W40.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W41.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W42.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W43.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W44.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W45.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W46.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W47.pkl\" dataframe...\n",
      "loading . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W48.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W49.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W50.pkl\" dataframe...\n",
      "loading . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W51.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n",
      "\n",
      "Training & Saving \"data_인공지능_W52.pkl\" dataframe...\n",
      "loading . . . . . . . . . \n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "fList = os.listdir(\"./\"+save_dir+\"/\")\n",
    "pkl_list = [file for file in fList if file.endswith(\".pkl\")]\n",
    "\n",
    "for i in range(len(pkl_list)):\n",
    "    print(\"\\nTraining & Saving \\\"{}\\\" dataframe...\".format(pkl_list[i]))\n",
    "    pklName = \"./\"+save_dir+\"/\"+pkl_list[i]\n",
    "    model_name = \"./res_word2vec/model_\" + pkl_list[i][5:-4]\n",
    "    model_save = \"./res_models/model_\"+ pkl_list[i][5:-4] + \".wv\"\n",
    "    df = pd.read_pickle(pklName)\n",
    "    result, model = model_train(df, 'contents')\n",
    "    create_tensors(model, model_name)\n",
    "    model.save(model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_금융_W01.wv',\n",
       " 'model_금융_W02.wv',\n",
       " 'model_금융_W03.wv',\n",
       " 'model_금융_W04.wv',\n",
       " 'model_금융_W05.wv',\n",
       " 'model_금융_W06.wv',\n",
       " 'model_금융_W07.wv',\n",
       " 'model_금융_W08.wv',\n",
       " 'model_금융_W09.wv',\n",
       " 'model_금융_W10.wv',\n",
       " 'model_금융_W11.wv',\n",
       " 'model_금융_W12.wv',\n",
       " 'model_금융_W13.wv',\n",
       " 'model_금융_W14.wv',\n",
       " 'model_금융_W15.wv',\n",
       " 'model_금융_W16.wv',\n",
       " 'model_금융_W17.wv',\n",
       " 'model_금융_W18.wv',\n",
       " 'model_금융_W19.wv',\n",
       " 'model_금융_W20.wv',\n",
       " 'model_금융_W21.wv',\n",
       " 'model_금융_W22.wv',\n",
       " 'model_금융_W23.wv',\n",
       " 'model_금융_W24.wv',\n",
       " 'model_금융_W25.wv',\n",
       " 'model_금융_W26.wv',\n",
       " 'model_금융_W27.wv',\n",
       " 'model_금융_W28.wv',\n",
       " 'model_금융_W29.wv',\n",
       " 'model_금융_W30.wv',\n",
       " 'model_금융_W31.wv',\n",
       " 'model_금융_W32.wv',\n",
       " 'model_금융_W33.wv',\n",
       " 'model_금융_W34.wv',\n",
       " 'model_금융_W35.wv',\n",
       " 'model_금융_W36.wv',\n",
       " 'model_금융_W37.wv',\n",
       " 'model_금융_W38.wv',\n",
       " 'model_금융_W39.wv',\n",
       " 'model_금융_W40.wv',\n",
       " 'model_금융_W41.wv',\n",
       " 'model_금융_W42.wv',\n",
       " 'model_금융_W43.wv',\n",
       " 'model_금융_W44.wv',\n",
       " 'model_금융_W45.wv',\n",
       " 'model_금융_W46.wv',\n",
       " 'model_금융_W47.wv',\n",
       " 'model_금융_W48.wv',\n",
       " 'model_금융_W49.wv',\n",
       " 'model_금융_W50.wv',\n",
       " 'model_금융_W51.wv',\n",
       " 'model_금융_W52.wv',\n",
       " 'model_산업_W01.wv',\n",
       " 'model_산업_W02.wv',\n",
       " 'model_산업_W03.wv',\n",
       " 'model_산업_W04.wv',\n",
       " 'model_산업_W05.wv',\n",
       " 'model_산업_W06.wv',\n",
       " 'model_산업_W07.wv',\n",
       " 'model_산업_W08.wv',\n",
       " 'model_산업_W09.wv',\n",
       " 'model_산업_W10.wv',\n",
       " 'model_산업_W11.wv',\n",
       " 'model_산업_W12.wv',\n",
       " 'model_산업_W13.wv',\n",
       " 'model_산업_W14.wv',\n",
       " 'model_산업_W15.wv',\n",
       " 'model_산업_W16.wv',\n",
       " 'model_산업_W17.wv',\n",
       " 'model_산업_W18.wv',\n",
       " 'model_산업_W19.wv',\n",
       " 'model_산업_W20.wv',\n",
       " 'model_산업_W21.wv',\n",
       " 'model_산업_W22.wv',\n",
       " 'model_산업_W23.wv',\n",
       " 'model_산업_W24.wv',\n",
       " 'model_산업_W25.wv',\n",
       " 'model_산업_W26.wv',\n",
       " 'model_산업_W27.wv',\n",
       " 'model_산업_W28.wv',\n",
       " 'model_산업_W29.wv',\n",
       " 'model_산업_W30.wv',\n",
       " 'model_산업_W31.wv',\n",
       " 'model_산업_W32.wv',\n",
       " 'model_산업_W33.wv',\n",
       " 'model_산업_W34.wv',\n",
       " 'model_산업_W35.wv',\n",
       " 'model_산업_W36.wv',\n",
       " 'model_산업_W37.wv',\n",
       " 'model_산업_W38.wv',\n",
       " 'model_산업_W39.wv',\n",
       " 'model_산업_W40.wv',\n",
       " 'model_산업_W41.wv',\n",
       " 'model_산업_W42.wv',\n",
       " 'model_산업_W43.wv',\n",
       " 'model_산업_W44.wv',\n",
       " 'model_산업_W45.wv',\n",
       " 'model_산업_W46.wv',\n",
       " 'model_산업_W47.wv',\n",
       " 'model_산업_W48.wv',\n",
       " 'model_산업_W49.wv',\n",
       " 'model_산업_W50.wv',\n",
       " 'model_산업_W51.wv',\n",
       " 'model_산업_W52.wv',\n",
       " 'model_인공지능_W01.wv',\n",
       " 'model_인공지능_W02.wv',\n",
       " 'model_인공지능_W03.wv',\n",
       " 'model_인공지능_W04.wv',\n",
       " 'model_인공지능_W05.wv',\n",
       " 'model_인공지능_W06.wv',\n",
       " 'model_인공지능_W07.wv',\n",
       " 'model_인공지능_W08.wv',\n",
       " 'model_인공지능_W09.wv',\n",
       " 'model_인공지능_W10.wv',\n",
       " 'model_인공지능_W11.wv',\n",
       " 'model_인공지능_W12.wv',\n",
       " 'model_인공지능_W13.wv',\n",
       " 'model_인공지능_W14.wv',\n",
       " 'model_인공지능_W15.wv',\n",
       " 'model_인공지능_W16.wv',\n",
       " 'model_인공지능_W17.wv',\n",
       " 'model_인공지능_W18.wv',\n",
       " 'model_인공지능_W19.wv',\n",
       " 'model_인공지능_W20.wv',\n",
       " 'model_인공지능_W21.wv',\n",
       " 'model_인공지능_W22.wv',\n",
       " 'model_인공지능_W23.wv',\n",
       " 'model_인공지능_W24.wv',\n",
       " 'model_인공지능_W25.wv',\n",
       " 'model_인공지능_W26.wv',\n",
       " 'model_인공지능_W27.wv',\n",
       " 'model_인공지능_W28.wv',\n",
       " 'model_인공지능_W29.wv',\n",
       " 'model_인공지능_W30.wv',\n",
       " 'model_인공지능_W31.wv',\n",
       " 'model_인공지능_W32.wv',\n",
       " 'model_인공지능_W33.wv',\n",
       " 'model_인공지능_W34.wv',\n",
       " 'model_인공지능_W35.wv',\n",
       " 'model_인공지능_W36.wv',\n",
       " 'model_인공지능_W37.wv',\n",
       " 'model_인공지능_W38.wv',\n",
       " 'model_인공지능_W39.wv',\n",
       " 'model_인공지능_W40.wv',\n",
       " 'model_인공지능_W41.wv',\n",
       " 'model_인공지능_W42.wv',\n",
       " 'model_인공지능_W43.wv',\n",
       " 'model_인공지능_W44.wv',\n",
       " 'model_인공지능_W45.wv',\n",
       " 'model_인공지능_W46.wv',\n",
       " 'model_인공지능_W47.wv',\n",
       " 'model_인공지능_W48.wv',\n",
       " 'model_인공지능_W49.wv',\n",
       " 'model_인공지능_W50.wv',\n",
       " 'model_인공지능_W51.wv',\n",
       " 'model_인공지능_W52.wv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = os.listdir(\"./res_models/\")\n",
    "model_list = [file for file in model_dir]\n",
    "\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('비실', 0.945334792137146), ('재외공관', 0.9068991541862488), ('공함', 0.8974473476409912), ('금융감독원', 0.89692223072052), ('눈치', 0.892565131187439), ('소프트웨어', 0.8902608156204224), ('누리집', 0.8896006941795349), ('공증', 0.8893685340881348), ('촉진', 0.8886565566062927), ('경리', 0.8819602131843567)]\n",
      "[('이야기', 0.9751126170158386), ('소개', 0.9686547517776489), ('전략', 0.9660241603851318), ('컴퓨터공학', 0.9660229682922363), ('계장', 0.9654173254966736), ('연세대', 0.9653378129005432), ('세기', 0.9640803337097168), ('이슈', 0.9624744057655334), ('과학', 0.9612370729446411), ('이지문', 0.9586098194122314)]\n",
      "[('예정', 0.9644852876663208), ('구축', 0.9579850435256958), ('선도', 0.9569998383522034), ('네트워크', 0.9546102285385132), ('플랫폼', 0.9534415602684021), ('활용', 0.9522559642791748), ('기반', 0.94450443983078), ('결제', 0.9440504312515259), ('출범', 0.9410536289215088), ('개요', 0.939410924911499)]\n",
      "[('파트너', 0.9571378231048584), ('샐러드', 0.94179368019104), ('콘텐츠', 0.9320996999740601), ('금융결제원', 0.9169362783432007), ('컨설팅', 0.9160155057907104), ('경주', 0.9157856702804565), ('스마트', 0.9125745296478271), ('션', 0.9123531579971313), ('신청', 0.9075490236282349), ('발굴', 0.9069557189941406)]\n",
      "[('팀', 0.9543936848640442), ('부서장', 0.9376723766326904), ('대우', 0.9367482662200928), ('정상훈', 0.9325650930404663), ('본부', 0.912148118019104), ('사부', 0.9117320775985718), ('신심', 0.9104040861129761), ('승진', 0.9056648015975952), ('부', 0.904369592666626), ('나일흠', 0.9012292623519897)]\n",
      "[('지속', 0.9897722005844116), ('혁신', 0.9897595047950745), ('보안', 0.987274169921875), ('샌드박스', 0.9800058007240295), ('안정', 0.974157452583313), ('자금세탁', 0.9723937511444092), ('기반', 0.9664799571037292), ('정비', 0.9647995233535767), ('방지', 0.9638195037841797), ('위해', 0.9632878303527832)]\n",
      "[('리서치', 0.9703118801116943), ('캠퍼스', 0.9656272530555725), ('부산', 0.9609570503234863), ('포렌', 0.9601621627807617), ('지역', 0.9581428170204163), ('진흥', 0.9570717811584473), ('기획', 0.9569656848907471), ('전북도', 0.9539106488227844), ('경남', 0.9537498950958252), ('조감도', 0.9522806406021118)]\n",
      "[('리테일', 0.9468274116516113), ('종합', 0.9378081560134888), ('체계', 0.9341183304786682), ('협업', 0.9325363039970398), ('개발', 0.93149733543396), ('구축', 0.925029993057251), ('상품화', 0.9133834838867188), ('토론회', 0.9117646217346191), ('협력', 0.9110497832298279), ('통한', 0.8993867635726929)]\n",
      "[('금융투자', 0.9346480369567871), ('교육', 0.9344707131385803), ('컨텐츠', 0.9334782958030701), ('현주', 0.9276859760284424), ('창립', 0.9243135452270508), ('기념', 0.9236162304878235), ('어린이', 0.922572135925293), ('학생', 0.9223942160606384), ('강사', 0.9221473932266235), ('장애', 0.915591299533844)]\n",
      "[('생태계', 0.91496741771698), ('역량', 0.8999926447868347), ('융합', 0.8957861661911011), ('결', 0.8943473100662231), ('접목', 0.89397794008255), ('공동', 0.8935486078262329), ('노하우', 0.8795516490936279), ('지능', 0.8762742280960083), ('맞춤', 0.8693580627441406), ('인공', 0.8674564957618713)]\n",
      "[('정보기술', 0.9409812092781067), ('모델', 0.9098260402679443), ('플랫폼', 0.8627800345420837), ('블록', 0.8621160984039307), ('서비스', 0.8550036549568176), ('활용', 0.8525317907333374), ('업그레이드', 0.8518049120903015), ('비대', 0.8513926267623901), ('개발', 0.8443607091903687), ('우대금리', 0.8427565693855286)]\n",
      "[('혁신', 0.9592349529266357), ('부문', 0.9501839876174927), ('조직', 0.9399369955062866), ('재선', 0.9299602508544922), ('총괄', 0.9269798398017883), ('우리금융', 0.9226955771446228), ('주도', 0.9198340177536011), ('연구소', 0.9077608585357666), ('기기', 0.8935635089874268), ('지주사', 0.8919140696525574)]\n",
      "[('플러스', 0.9140885472297668), ('린지', 0.9049099683761597), ('프로그램', 0.9041815996170044), ('데모', 0.8577698469161987), ('기', 0.8530861735343933), ('제공', 0.849226713180542), ('캠퍼스', 0.8383944034576416), ('우리금융', 0.8308672904968262), ('데이', 0.8266969919204712), ('스타트업', 0.8232571482658386)]\n",
      "[('개발', 0.9410097599029541), ('기술', 0.9235824346542358), ('혁신', 0.9192794561386108), ('일반', 0.9103209376335144), ('사물인터넷', 0.9009686708450317), ('인공', 0.9004429578781128), ('지능', 0.9003980159759521), ('핵심', 0.8945547342300415), ('결', 0.8923259973526001), ('투자자', 0.8917603492736816)]\n",
      "[('육성', 0.9422576427459717), ('진출', 0.9289802312850952), ('인재', 0.9035130739212036), ('사업', 0.901268720626831), ('조성', 0.8989409804344177), ('동력', 0.8982449769973755), ('구축', 0.8974109888076782), ('생태계', 0.8961083292961121), ('청년', 0.8934774994850159), ('창업', 0.8901236057281494)]\n",
      "[('변화', 0.9790157675743103), ('발대식', 0.9702514410018921), ('혁신', 0.9659908413887024), ('자리', 0.9593141674995422), ('일조', 0.9588662981987), ('김태오', 0.9531633257865906), ('마인드', 0.9507116079330444), ('리더', 0.9457206726074219), ('고민', 0.9384957551956177), ('노력', 0.9377471208572388)]\n",
      "[('밀착', 0.9580274820327759), ('음성인식', 0.9555763006210327), ('고객', 0.9547380805015564), ('통합', 0.9444504976272583), ('만족', 0.9345101714134216), ('사적', 0.9341096878051758), ('어르신', 0.931442379951477), ('원스톱', 0.9303442239761353), ('아만다', 0.927008867263794), ('선포', 0.9203730225563049)]\n",
      "[('카카오', 0.9872421622276306), ('추가', 0.976445198059082), ('데이터', 0.973292887210846), ('예방', 0.9692882895469666), ('교류', 0.9660015106201172), ('신분증', 0.9635457992553711), ('전문', 0.9628344774246216), ('펌뱅킹', 0.9602739810943604), ('채무자', 0.9584767818450928), ('앱', 0.9580331444740295)]\n",
      "[('인재', 0.925493597984314), ('특성화고', 0.9152288436889648), ('백지영', 0.8422490358352661), ('부금', 0.8382917642593384), ('육성', 0.8349913358688354), ('프로젝트', 0.8265645503997803), ('구매', 0.824099600315094), ('비대', 0.8218659162521362), ('일리', 0.8192013502120972), ('본점', 0.818233847618103)]\n",
      "[('디지털화', 0.9836269021034241), ('콘텐츠', 0.9826553463935852), ('강연', 0.9791238903999329), ('여행', 0.9784878492355347), ('비트코인', 0.9782346487045288), ('녹취', 0.9776988625526428), ('창규', 0.977092444896698), ('레저', 0.9769238233566284), ('여가', 0.9768361449241638), ('기업인', 0.9765613675117493)]\n",
      "[('트랜스', 0.9714165925979614), ('텔레콤', 0.9512943029403687), ('비즈니스', 0.945193350315094), ('메이', 0.9446314573287964), ('기업은행', 0.9429323673248291), ('원기', 0.9332209825515747), ('대표', 0.9281682968139648), ('션', 0.9146915674209595), ('이사장', 0.9084275960922241), ('생태계', 0.9032860994338989)]\n",
      "[('감성', 0.8983771204948425), ('콘텐츠', 0.8816654682159424), ('축하', 0.878754734992981), ('텔링크', 0.8737629652023315), ('적응력', 0.8731167316436768), ('캠퍼스', 0.8715665936470032), ('하나은행', 0.8652007579803467), ('부사', 0.8641101121902466), ('트랜스', 0.8587529063224792), ('마인드', 0.8583968877792358)]\n",
      "[('금융', 0.9510738849639893), ('명제', 0.9496543407440186), ('준성', 0.9285834431648254), ('시대', 0.9210625886917114), ('지향', 0.9080572724342346), ('총괄', 0.8999183177947998), ('독자', 0.8960837125778198), ('전제', 0.8958452939987183), ('티아이', 0.8900443315505981), ('사의', 0.8882359266281128)]\n",
      "[('제안', 0.9816904067993164), ('무대', 0.9785212278366089), ('원윤', 0.9780208468437195), ('앱', 0.9748650789260864), ('우버', 0.974629282951355), ('메세지', 0.9744687080383301), ('오피스', 0.9735377430915833), ('제작', 0.9730735421180725), ('미래에셋', 0.9728509783744812), ('종규', 0.9713054895401001)]\n",
      "[('타', 0.9792141914367676), ('지안', 0.9758111834526062), ('이지윤', 0.9726135730743408), ('길재', 0.9715902805328369), ('오픈', 0.9709582328796387), ('웹페이지', 0.9704384803771973), ('설정', 0.96961510181427), ('크립', 0.9674915075302124), ('랙티브', 0.9662271738052368), ('도시', 0.965948760509491)]\n",
      "[('수용', 0.9280301332473755), ('시니어', 0.9233294129371643), ('대우', 0.9035382270812988), ('교육', 0.9007828235626221), ('고객', 0.8968040943145752), ('국민은행', 0.8913261294364929), ('피해', 0.8904218077659607), ('예방', 0.8868240118026733), ('계층', 0.885699987411499), ('생', 0.8803823590278625)]\n",
      "[('빅데이터', 0.8935434818267822), ('발', 0.8874202966690063), ('지능', 0.8633915781974792), ('접속', 0.8628960847854614), ('인공', 0.8611575365066528), ('플레이', 0.8595763444900513), ('모바일', 0.8545551300048828), ('교류', 0.8533111810684204), ('미술', 0.8527289628982544), ('라이브', 0.851582944393158)]\n",
      "[('서비스', 0.9858587980270386), ('비대', 0.9818341135978699), ('담당', 0.9623384475708008), ('창의', 0.9587055444717407), ('플랫폼', 0.9468657374382019), ('분석', 0.9466632604598999), ('거래', 0.9408101439476013), ('데이터', 0.9355028867721558), ('인공', 0.9340685606002808), ('센터', 0.933686375617981)]\n",
      "[('상호', 0.9452575445175171), ('수혈', 0.9409158229827881), ('개발', 0.9319555759429932), ('변화', 0.9307317733764648), ('은혁', 0.9289100170135498), ('산업', 0.9277832508087158), ('체질', 0.9272652864456177), ('팀', 0.9266213774681091), ('십분', 0.9262985587120056), ('노하우', 0.9257335662841797)]\n",
      "[('전략', 0.9508546590805054), ('이창', 0.921104371547699), ('전무', 0.9190504550933838), ('남종', 0.9142318964004517), ('기획', 0.9082199931144714), ('본부', 0.9063992500305176), ('총괄', 0.9062715172767639), ('이대원', 0.9056352972984314), ('효', 0.9036331176757812), ('상무', 0.8950557112693787)]\n",
      "[('비즈니스', 0.9697964787483215), ('통한', 0.9645951986312866), ('강화', 0.9580461382865906), ('경쟁력', 0.9564324617385864), ('발굴', 0.9541394710540771), ('영역', 0.9421032071113586), ('글로벌', 0.9376704692840576), ('핵심', 0.935756266117096), ('수립', 0.9343762397766113), ('부문', 0.9315916895866394)]\n",
      "[('전문직', 0.971714973449707), ('원격', 0.9608575105667114), ('본점', 0.9581334590911865), ('온라인', 0.9560019373893738), ('실시간', 0.9518346190452576), ('퇴직', 0.9513869881629944), ('다변', 0.9489591121673584), ('신청', 0.9424938559532166), ('연금', 0.9233372211456299), ('기업은행', 0.9230643510818481)]\n",
      "[('워킹', 0.9638439416885376), ('협력', 0.950744092464447), ('협업', 0.9443173408508301), ('상호', 0.9391109943389893), ('교육', 0.9380935430526733), ('경험', 0.9370769262313843), ('비즈니스', 0.9341841340065002), ('기술', 0.9307919144630432), ('통한', 0.9304054975509644), ('발전', 0.928741455078125)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('재박', 0.8966090679168701), ('황인', 0.8098844885826111), ('전환', 0.8055713176727295), ('국', 0.7942383885383606), ('이진수', 0.7742668986320496), ('이석주', 0.7688097953796387), ('장경', 0.7668104767799377), ('최우현', 0.7611579895019531), ('방창용', 0.7579959630966187), ('삼정', 0.754102349281311)]\n",
      "[('김정훈', 0.9675396084785461), ('창공', 0.9626494646072388), ('시너지', 0.9608137607574463), ('구로', 0.9535019397735596), ('김종민', 0.9530425071716309), ('볼트', 0.9519714713096619), ('유재', 0.9510033130645752), ('박석', 0.9497573971748352), ('마', 0.9495152235031128), ('무부', 0.9490014314651489)]\n",
      "[('트랜스', 0.9041216373443604), ('포', 0.8584372401237488), ('테크', 0.8572119474411011), ('핀', 0.8515437245368958), ('한화', 0.8514082431793213), ('화생', 0.8494566679000854), ('메이', 0.8452227115631104), ('실', 0.834713339805603), ('환경', 0.8227214813232422), ('기술', 0.803566038608551)]\n",
      "[('혁신', 0.9282957911491394), ('휴먼', 0.9014040231704712), ('인재', 0.8903398513793945), ('검토', 0.858673095703125), ('생태계', 0.8514482378959656), ('회', 0.8491668105125427), ('도전', 0.8456653952598572), ('기기', 0.8442131876945496), ('역량', 0.8435429930686951), ('전환', 0.8397862911224365)]\n",
      "[('인포', 0.9576570987701416), ('은상', 0.9367218613624573), ('개발', 0.9343869686126709), ('데이터', 0.9316694736480713), ('선정', 0.9286388158798218), ('미래에셋', 0.91767817735672), ('납', 0.9110919833183289), ('하반기', 0.9093742370605469), ('협업', 0.908367931842804), ('중심지', 0.9065882563591003)]\n",
      "[('볼', 0.9725745916366577), ('기획', 0.9694678783416748), ('일리', 0.9683178067207336), ('닷넷', 0.9658352136611938), ('전문', 0.9590954780578613), ('노컷', 0.9582217931747437), ('조세일보', 0.9556704759597778), ('라이트', 0.9554773569107056), ('남극', 0.9534681439399719), ('회원', 0.9525700807571411)]\n",
      "[('운동', 0.9490164518356323), ('인식', 0.934188723564148), ('개발', 0.9328659176826477), ('인공', 0.9267177581787109), ('집', 0.9252854585647583), ('지능', 0.9212380051612854), ('표준', 0.9210444688796997), ('검수', 0.9190912842750549), ('자연어', 0.9181117415428162), ('공통', 0.9158127307891846)]\n",
      "[('애플리케이션', 0.9847033023834229), ('챗봇', 0.9783186912536621), ('세븐', 0.9767211079597473), ('홈페이지', 0.9764419198036194), ('전자', 0.9750750064849854), ('고지', 0.9683342576026917), ('문서', 0.9661489725112915), ('알뜰폰', 0.9659008979797363), ('모바일', 0.9655940532684326), ('앱', 0.9650360941886902)]\n",
      "[('전문성', 0.9396985173225403), ('환경', 0.9139025211334229), ('능력', 0.8984790444374084), ('혁신', 0.897649347782135), ('개시', 0.8853789567947388), ('시설', 0.883246898651123), ('수습', 0.8529577255249023), ('겸비', 0.8508189916610718), ('내지', 0.8466789722442627), ('사업', 0.846355676651001)]\n",
      "[('교육', 0.9028421640396118), ('카이스트', 0.8877795934677124), ('과학기술원', 0.8804329633712769), ('전문', 0.8766818046569824), ('양성', 0.8720533847808838), ('선정', 0.8557430505752563), ('제공', 0.834062397480011), ('인력', 0.8330332040786743), ('교육과정', 0.828418493270874), ('딥', 0.809506893157959)]\n",
      "[('분산', 0.9565548896789551), ('보안', 0.9475693702697754), ('신원', 0.9474172592163086), ('기반', 0.9464635252952576), ('현물', 0.9428204298019409), ('개요', 0.9379812479019165), ('기술', 0.9348160624504089), ('비대', 0.9329898357391357), ('앱', 0.931699812412262), ('프레임워크', 0.9301643371582031)]\n",
      "[('선행', 0.9493560194969177), ('개선', 0.9424169063568115), ('안전벨트', 0.9423598051071167), ('률', 0.9410355091094971), ('메이드', 0.9371060729026794), ('임병철', 0.936536431312561), ('정', 0.9349669814109802), ('촉발', 0.9346356987953186), ('통신', 0.9341480135917664), ('공통점', 0.9338366389274597)]\n",
      "[('통해', 0.9671655893325806), ('분야', 0.963970422744751), ('우수', 0.9466806054115295), ('특화', 0.933286190032959), ('클럽', 0.932710587978363), ('기후변화', 0.9326630234718323), ('앱', 0.9319111108779907), ('선도', 0.9307393431663513), ('발굴', 0.9262242913246155), ('체', 0.925826907157898)]\n",
      "[('비도', 0.9062005281448364), ('기가스', 0.8963150978088379), ('축', 0.8958591222763062), ('물건', 0.8949917554855347), ('고도화', 0.8879441022872925), ('과제', 0.8847692012786865), ('토목', 0.8843312859535217), ('별', 0.8793940544128418), ('지출', 0.8765246868133545), ('월급', 0.8693838715553284)]\n",
      "[('급변', 0.9288691878318787), ('양성', 0.9164319038391113), ('수원', 0.9153154492378235), ('인력', 0.9093137979507446), ('교육과정', 0.9089577198028564), ('실습', 0.885654091835022), ('데이터', 0.8757643699645996), ('전문', 0.8742026090621948), ('활용', 0.8668467402458191), ('아카데미', 0.8648791909217834)]\n",
      "[('여의도', 0.9800443053245544), ('대학원', 0.9641874432563782), ('카이스트', 0.9515433311462402), ('과정', 0.9506431221961975), ('융', 0.9454071521759033), ('금융', 0.9338690042495728), ('개설', 0.9331662654876709), ('보안', 0.9312347173690796), ('대학', 0.9309767484664917), ('전문가', 0.9254404306411743)]\n",
      "[('모범', 0.9177631139755249), ('포스트', 0.8946269750595093), ('전환', 0.8845281004905701), ('축사', 0.8751086592674255), ('주제', 0.8741405010223389), ('비대', 0.8654079437255859), ('전략', 0.8514599800109863), ('양해각서', 0.8506214618682861), ('원장', 0.8482750654220581), ('기회', 0.8386249542236328)]\n",
      "[('블루', 0.8983033895492554), ('대응', 0.8869941234588623), ('백경호', 0.875007152557373), ('기술', 0.8739159107208252), ('기보', 0.8735524415969849), ('우리은행', 0.8698973655700684), ('발굴', 0.8696044683456421), ('성장기', 0.8628816604614258), ('혁신', 0.8599731922149658), ('포스트', 0.8589136600494385)]\n",
      "[('클라우드', 0.9051297903060913), ('빙', 0.8989826440811157), ('기반', 0.8702064752578735), ('페이', 0.8660452365875244), ('멤버십', 0.8475925922393799), ('언택트', 0.8453941345214844), ('거듭', 0.8412711024284363), ('준수', 0.8403466939926147), ('예정', 0.8368459939956665), ('뱅킹', 0.8320568203926086)]\n"
     ]
    }
   ],
   "source": [
    "def print_similar(model_, query):\n",
    "    try:\n",
    "        model_result = model_.wv.most_similar(query)\n",
    "        print(model_result)\n",
    "        return model_result\n",
    "    except KeyError:\n",
    "        print('{} not in model vocabulary. Please try another query.'.format(query))\n",
    "\n",
    "        \n",
    "finance_models = []\n",
    "for i in range(52):\n",
    "    model_temp = gensim.models.Word2Vec.load('./res_models/{}'.format(model_list[i]))\n",
    "    finance_models.append(print_similar(model_temp, \"디지털\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\AppData\\Local\\Continuum\\anaconda3\\envs\\data_science\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "test_model = gensim.models.Word2Vec.load('./res_models/model_인공지능_W52.wv')\n",
    "\n",
    "vocab = list(test_model.wv.vocab)\n",
    "X = test_model[vocab]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df_temp = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'x'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-940f5493f786>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'x'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('트랜스', 0.9288039207458496), ('포', 0.9179084300994873), ('메이', 0.8913238048553467), ('션', 0.8782243728637695), ('뉴딜', 0.8766234517097473), ('선제', 0.845299243927002), ('대응', 0.8289932012557983), ('차원', 0.8152667284011841), ('성장', 0.7932179570198059), ('휴먼', 0.7921728491783142)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('트랜스', 0.9288039207458496),\n",
       " ('포', 0.9179084300994873),\n",
       " ('메이', 0.8913238048553467),\n",
       " ('션', 0.8782243728637695),\n",
       " ('뉴딜', 0.8766234517097473),\n",
       " ('선제', 0.845299243927002),\n",
       " ('대응', 0.8289932012557983),\n",
       " ('차원', 0.8152667284011841),\n",
       " ('성장', 0.7932179570198059),\n",
       " ('휴먼', 0.7921728491783142)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_similar(test_model, '디지털')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>week_num</th>\n",
       "      <th>word_similar</th>\n",
       "      <th>similar_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>줌</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>원격수업</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>원격진료</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>온라인</td>\n",
       "      <td>[(판매, 0.9950112104415894), (이처럼, 0.99011099338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>클라우드</td>\n",
       "      <td>[(요소, 0.9829429388046265), (상, 0.9808036088943...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query week_num word_similar  \\\n",
       "0    금융      W01            줌   \n",
       "1    금융      W01         원격수업   \n",
       "2    금융      W01         원격진료   \n",
       "3    금융      W01          온라인   \n",
       "4    금융      W01         클라우드   \n",
       "\n",
       "                                       similar_words  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3  [(판매, 0.9950112104415894), (이처럼, 0.99011099338...  \n",
       "4  [(요소, 0.9829429388046265), (상, 0.9808036088943...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"word_similar_list.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>week_num</th>\n",
       "      <th>word_similar</th>\n",
       "      <th>similar_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>온라인</td>\n",
       "      <td>[(판매, 0.9950112104415894), (이처럼, 0.99011099338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>클라우드</td>\n",
       "      <td>[(요소, 0.9829429388046265), (상, 0.9808036088943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>모바일</td>\n",
       "      <td>[(한경닷컴, 0.9931442141532898), (여름, 0.9930543899...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>빅데이터</td>\n",
       "      <td>[(인프라, 0.9909763336181641), (구축, 0.97108638286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>딥러닝</td>\n",
       "      <td>[(모형, 0.9857240319252014), (예측, 0.982086122035...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query week_num word_similar  \\\n",
       "3     금융      W01          온라인   \n",
       "4     금융      W01         클라우드   \n",
       "5     금융      W01          모바일   \n",
       "6     금융      W01         빅데이터   \n",
       "12    금융      W01          딥러닝   \n",
       "\n",
       "                                        similar_words  \n",
       "3   [(판매, 0.9950112104415894), (이처럼, 0.99011099338...  \n",
       "4   [(요소, 0.9829429388046265), (상, 0.9808036088943...  \n",
       "5   [(한경닷컴, 0.9931442141532898), (여름, 0.9930543899...  \n",
       "6   [(인프라, 0.9909763336181641), (구축, 0.97108638286...  \n",
       "12  [(모형, 0.9857240319252014), (예측, 0.982086122035...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>week_num</th>\n",
       "      <th>word_similar</th>\n",
       "      <th>similar_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>온라인</td>\n",
       "      <td>[(판매, 0.9950112104415894), (이처럼, 0.99011099338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>클라우드</td>\n",
       "      <td>[(요소, 0.9829429388046265), (상, 0.9808036088943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>모바일</td>\n",
       "      <td>[(한경닷컴, 0.9931442141532898), (여름, 0.9930543899...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>빅데이터</td>\n",
       "      <td>[(인프라, 0.9909763336181641), (구축, 0.97108638286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>금융</td>\n",
       "      <td>W01</td>\n",
       "      <td>딥러닝</td>\n",
       "      <td>[(모형, 0.9857240319252014), (예측, 0.982086122035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>금융</td>\n",
       "      <td>W52</td>\n",
       "      <td>온라인</td>\n",
       "      <td>[(자연계, 0.9682503342628479), (손해, 0.90417140722...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>금융</td>\n",
       "      <td>W52</td>\n",
       "      <td>클라우드</td>\n",
       "      <td>[(안전성, 0.934285581111908), (기반, 0.931890487670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>금융</td>\n",
       "      <td>W52</td>\n",
       "      <td>모바일</td>\n",
       "      <td>[(앱, 0.9013203978538513), (자동, 0.9008282423019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>금융</td>\n",
       "      <td>W52</td>\n",
       "      <td>빅데이터</td>\n",
       "      <td>[(대조, 0.9151080846786499), (뱅킹, 0.896952152252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>금융</td>\n",
       "      <td>W52</td>\n",
       "      <td>데이터</td>\n",
       "      <td>[(포럼, 0.9280499815940857), (분야, 0.919787526130...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     query week_num word_similar  \\\n",
       "3       금융      W01          온라인   \n",
       "4       금융      W01         클라우드   \n",
       "5       금융      W01          모바일   \n",
       "6       금융      W01         빅데이터   \n",
       "12      금융      W01          딥러닝   \n",
       "...    ...      ...          ...   \n",
       "1839    금융      W52          온라인   \n",
       "1840    금융      W52         클라우드   \n",
       "1841    금융      W52          모바일   \n",
       "1842    금융      W52         빅데이터   \n",
       "1849    금융      W52          데이터   \n",
       "\n",
       "                                          similar_words  \n",
       "3     [(판매, 0.9950112104415894), (이처럼, 0.99011099338...  \n",
       "4     [(요소, 0.9829429388046265), (상, 0.9808036088943...  \n",
       "5     [(한경닷컴, 0.9931442141532898), (여름, 0.9930543899...  \n",
       "6     [(인프라, 0.9909763336181641), (구축, 0.97108638286...  \n",
       "12    [(모형, 0.9857240319252014), (예측, 0.982086122035...  \n",
       "...                                                 ...  \n",
       "1839  [(자연계, 0.9682503342628479), (손해, 0.90417140722...  \n",
       "1840  [(안전성, 0.934285581111908), (기반, 0.931890487670...  \n",
       "1841  [(앱, 0.9013203978538513), (자동, 0.9008282423019...  \n",
       "1842  [(대조, 0.9151080846786499), (뱅킹, 0.896952152252...  \n",
       "1849  [(포럼, 0.9280499815940857), (분야, 0.919787526130...  \n",
       "\n",
       "[260 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('week_num').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
