{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 비슷한 단어 모델 학습\n",
    "\n",
    "1. DataFrame을 만든다 -> crawling을 하던 (crawl_query) 또는 저장된 data를 불러와서 dataframe으로 만든다\n",
    "2. model_train을 한다. -> 결과: tokenized 단어 list + word2vec학습된 모델\n",
    "3. create_tensors를 하면 모델을 tensor로 바꾸고 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import gensim\n",
    "from gensim import utils\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def word2vec2tensor(word2vec_model, tensor_filename, binary=False):\n",
    "    \"\"\"Convert file in Word2Vec format and writes two files 2D tensor TSV file.\n",
    "\n",
    "    File \"tensor_filename\"_tensor.tsv contains word-vectors, \"tensor_filename\"_metadata.tsv contains words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word2vec_model_path : str\n",
    "        Path to file in Word2Vec format.\n",
    "    tensor_filename : str\n",
    "        Prefix for output files.\n",
    "    binary : bool, optional\n",
    "        True if input file in binary format.\n",
    "\n",
    "    \"\"\"\n",
    "    model = word2vec_model\n",
    "    outfiletsv = tensor_filename + '_tensor.tsv'\n",
    "    outfiletsvmeta = tensor_filename + '_metadata.tsv'\n",
    "\n",
    "    with utils.open(outfiletsv, 'wb') as file_vector, utils.open(outfiletsvmeta, 'wb') as file_metadata:\n",
    "        for word in model.index2word:\n",
    "            file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\\n'))\n",
    "            vector_row = '\\t'.join(str(x) for x in model[word])\n",
    "            file_vector.write(gensim.utils.to_utf8(vector_row) + gensim.utils.to_utf8('\\n'))\n",
    "\n",
    "    logger.info(\"2D tensor file saved to %s\", outfiletsv)\n",
    "    logger.info(\"Tensor metadata file saved to %s\", outfiletsvmeta)\n",
    "    \n",
    "def model_train(dataframe, content_col, size=100, window=5):\n",
    "    okt=Okt()\n",
    "    result = []\n",
    "    print('loading ', end='')\n",
    "    for index, row, in df.iterrows():\n",
    "        if (index % 1000 == 0):\n",
    "            print('. ', end='')\n",
    "        try:\n",
    "            tokenlist = okt.pos(row[content_col], stem=True, norm=True) # 단어 토큰화\n",
    "        except:\n",
    "            print(index, end='')\n",
    "            continue\n",
    "        temp=[]\n",
    "        for word in tokenlist:\n",
    "            if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "                temp.append((word[0])) # 해당 단어를 저장함\n",
    "\n",
    "        if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "            result.append(temp) # 결과에 저장\n",
    "    print('\\n\\nFinished!')\n",
    "    \n",
    "    model = Word2Vec(result, size=size, window=window, min_count=5, workers=4, sg=0)\n",
    "    return result, model\n",
    "\n",
    "def print_similar(model_, query):\n",
    "    try:\n",
    "        model_result = model_.wv.most_similar(query)\n",
    "        print(model_result)\n",
    "    except KeyError:\n",
    "        print('{} not in model vocabulary. Please try another query.'.format(query))\n",
    "    \n",
    "def create_tensors(model, output):\n",
    "    word2vec2tensor(model.wv, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def crawler(maxpage,query,s_date,e_date):\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1 # 11= 2페이지 21=3페이지 31=4페이지 ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    #f = open(RESULT_PATH + filename, 'w', encoding='utf-8-sig')\n",
    "    df = pd.DataFrame(columns=['date', 'title', 'contents'])\n",
    "    results_list = []\n",
    "\n",
    "    while page < maxpage_t:\n",
    "        print(page, 'loading', end='')\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        req = requests.get(url)\n",
    "        # print(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        #print(soup)\n",
    "        for urls in soup.select(\"._sp_each_url\"):\n",
    "            try :\n",
    "                #print(urls[\"href\"])\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    #print(urls[\"href\"])\n",
    "                    news_detail = get_news(urls[\"href\"])\n",
    "                    # pdate, pcompany, title, btext\n",
    "                    results_list.append((news_detail[1], news_detail[0], news_detail[2])) # date, title, contents\n",
    "                    #f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(news_detail[1], news_detail[4], news_detail[0], news_detail[2],news_detail[3])) # new style\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "        page += 10\n",
    "    df = pd.DataFrame(results_list, columns=['date', 'title', 'contents'])\n",
    "    print('\\nFinished!\\n')\n",
    "    return df\n",
    "\n",
    "def get_news(n_url):\n",
    "    print('.', end='')\n",
    "    news_detail = []\n",
    "    breq = requests.get(n_url)\n",
    "    bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "    title = bsoup.select('h3#articleTitle')[0].text #대괄호는 h3#articleTitle 인 것중 첫번째 그룹만 가져오겠다.\n",
    "    news_detail.append(title)\n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:11]\n",
    "    news_detail.append(pdate)\n",
    "    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "    news_detail.append(btext.strip())\n",
    "    news_detail.append(n_url)\n",
    "    pcompany = bsoup.select('#footer address')[0].a.get_text()\n",
    "    news_detail.append(pcompany)\n",
    "    # news_detail [title, pdate, btext, n_url, pcompany]\n",
    "    return news_detail\n",
    "\n",
    "def crawl_query(max_pages, query,s_date='2020.06.01', e_date='2020.06.02'):\n",
    "    maxpage = str(max_pages)\n",
    "    return crawler(maxpage,query,s_date,e_date)\n",
    "\n",
    "def crawl_n_save(max_pages, query,s_date='2020.06.01', e_date='2020.06.02'):\n",
    "    maxpage = str(max_pages)\n",
    "    df_temp = crawler(maxpage,query,s_date,e_date)\n",
    "    df_temp.to_pickle(\"./data_{}.pkl\".format(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loading............................\n",
      "Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = crawl_query(2, '코로나')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading . \n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "result, model = model_train(df, 'contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('코로나', 0.9972690939903259), ('백신', 0.9965233206748962), ('증가', 0.9962301850318909), ('물가', 0.996173620223999), ('중국', 0.9961512684822083), ('개발', 0.9961411952972412), ('이용', 0.9958362579345703), ('시간', 0.9957759380340576), ('진단', 0.9957061409950256), ('미국', 0.9956570267677307)]\n"
     ]
    }
   ],
   "source": [
    "print_similar(model, '감염')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('data_science': conda)",
   "language": "python",
   "name": "python37664bitdatasciencecondaac8662d8196444fe92c6fd699323ab1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
